Text Summarization using BiLSTM, Encoder-Decoder Architecture, and BERT Transformer

âœ¨ Project Overview

Customer reviews, especially on e-commerce platforms, can be lengthy and time-consuming to analyze manually. This project focuses on abstractive text summarization of customer reviews, aiming to generate concise summaries of long review texts automatically.

We leverage two approaches:
	â€¢	Custom BiLSTM Encoder-Decoder Architecture: Built using TensorFlow/Keras for generating summaries.
	â€¢	Pre-trained BERT Transformer (BART): Utilized through Hugging Faceâ€™s Transformers library for comparison and enhanced performance.

The project uses the Amazon Fine Food Reviews dataset sourced from Kaggle.

â¸»

ğŸ“š Problem Statement
	â€¢	Understand the concept of text summarization.
	â€¢	Perform thorough data cleaning and preprocessing.
	â€¢	Implement abstractive summarization using deep learning architectures.
	â€¢	Compare custom models against state-of-the-art transformers.

â¸»

ğŸ› ï¸ Technologies Used
	â€¢	Python
	â€¢	TensorFlow / Keras
	â€¢	Hugging Face Transformers
	â€¢	Pandas, NumPy, Matplotlib, Seaborn (for data manipulation and visualization)

â¸»

ğŸ”— Dataset

We have used the Amazon Fine Food Reviews dataset available on Kaggle.
This dataset contains 500,000+ food reviews including text, ratings, and summary fields.

â¸»

ğŸ—ï¸ Project Pipeline
	1.	Data Collection:
Downloaded using Kaggle API.
	2.	Data Cleaning:
	â€¢	Removed duplicates and NaN values.
	â€¢	Converted all text to lowercase.
	â€¢	Removed HTML tags, special characters, numbers, and text within parentheses.
	â€¢	Added special tokens (sostok, eostok) to summaries.
	3.	Text Preprocessing:
	â€¢	Tokenization.
	â€¢	Sequence padding.
	â€¢	Train-validation split.
	4.	Model Building:
	â€¢	BiLSTM Encoder: Processes input review text.
	â€¢	Decoder with LSTM: Predicts the summary sequence.
	â€¢	Embedding Layer: Learned word representations.
	â€¢	Early Stopping: To avoid overfitting.
	5.	Training:
	â€¢	Trained the BiLSTM model for 20 epochs with validation monitoring.
	6.	Inference:
	â€¢	Built separate encoder and decoder models for generating summaries.
	7.	Transformer Summarization:
	â€¢	Used BART (facebook/bart-large-cnn) from Hugging Face for benchmark summarization.

â¸»

ğŸ“ˆ Sample Results
	â€¢	Input: â€œI recently purchased this organic green tea, and I must say, I am thoroughly impressedâ€¦â€
	â€¢	Generated Summary (BiLSTM Model): great tea
	â€¢	Generated Summary (BART Model): â€œI drink this tea every morning, and it gives me a calming start to my dayâ€¦â€

â¸»

ğŸ‘¨â€ğŸ’» Authors
	â€¢	Vaishnav Naik
	â€¢	Yashaurya Soni
	â€¢	Piyush Borakhade

â¸»

ğŸ“„ License

This project is for academic purposes only.
